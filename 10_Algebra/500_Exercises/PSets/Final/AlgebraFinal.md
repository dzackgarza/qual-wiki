---
title: Final Exam
---


# 1
We prove a slightly stronger statement, namely:

**Theorem:**
$\ZZ$ is initial in the category of unital rings and ring homomorphisms.

This means that if we are given any such ring $R$, there is exactly one map $\ZZ \to R$.

Then, given an abelian group $A$, we can take $R = \hom_{\text{Ab}}(A, A)$, the hom set of abelian group endomorphisms, which is itself a unital ring.
This will imply that there is a unique map $\ZZ \to \hom_{\text{Ab}}(A, A)$, and since all such maps induce $\ZZ\dash$module structures on $A$, the result will follow.

*Proof:*
Let $R$ be arbitrary and $1_R$ be its multiplicative identity.
We first show that there exists a ring homomorphism $\ZZ \to R$, namely
\begin{align*}
\phi: \ZZ &\to R \\
n &\mapsto \sum_{i=1}^n 1_R
.\end{align*}

Note that $\phi(1) = 1_R$ and $\phi(-1) = -1_R$, and it is routine to check that $\phi$ is a ring homomorphism.

Now toward a contradiction, suppose there were another such ring homomorphism $\psi: \ZZ \to R$.
From the definition of a ring homomorphism, $\psi$ must satisfy,

\begin{align*}
\psi(1) &= 1_R \\
\psi(-1) &= -1_R
,\end{align*}

and by $\ZZ\dash$linearity, we must have 
$$
\psi(n) = \psi(\sum_{i=1}^n 1) = \sum_{i=1}^n \psi(1) = \sum_{i=1}^n 1_R = \phi(n),
$$

and so $\psi(x) = \phi(x)$ for every $x\in \ZZ$.
But this precisely means that $\psi = \phi$ as ring homomorphisms.
$\qed$



# 2

## a 

Let $\phi: \ZZ^4 \to \ZZ^3$ be a linear map which in the standard basis $\mathcal B$ is represented by

\begin{align*}
T &\definedas [\phi]_{\mathcal B} = 
[f_1^t, f_2^t, f_3^t, f_4^t] = 
\left[\begin{array}{cccc}
1  & 2  & 0 & 3 \\
0  & -3 & 3 & 1 \\
-1 & 1  & 1 & 5
\end{array}\right]
.\end{align*}


Then $\im T = \spanof_\ZZ\theset{f_1 ,f_2, f_3, f_4} \definedas N$ by construction.

We can then compute the echelon form

\begin{align*}
\left(\begin{array}{cccc}
1 & 1 & 1 & 5 \\
0 & 3 & 1 & 8 \\
0 & 0 & 4 & 9
\end{array}\right)
,\end{align*}

which has pivots in columns $1,2,$ and $3$, and thus

$$
N = \spanof_\ZZ\theset{f_1, f_2, f_3}
$$

## b

Without loss of generality, we can consider the image of the reduced matrix

\begin{align*}
A' =
\left(\begin{array}{ccc}
-1 & 2 & 0 \\
0 & -3 & 3 \\
1 & 1 & 1
\end{array}\right)
,\end{align*}

since $N = \im A = \im A'$.

When computing the characteristic polynomial, we find that $\chi_{A'}(x) = (x+3)(x+2)(x-2)$, which means that $A'$ has distinct eigenvalues.
We can thus immediately write

\begin{align*}
JCF(A) = 
\left[\begin{array}{c|c|c}
2 & 0 & 0 \\
\hline
0 & -2 & 0 \\
\hline
0 & 0 & -3
\end{array}\right]
.\end{align*}

From this, we can obtain the Smith normal form,


\begin{align*}
SNF(A') = 
\left[\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 12 \\
\end{array}\right]
,\end{align*}

which allows us to read off

\begin{align*}
\im A' \cong \ZZ \oplus \ZZ \oplus 12\ZZ 
,\end{align*}

and thus

\begin{align*}
\ZZ^3/N \cong \frac{\ZZ \oplus \ZZ \oplus \ZZ}{\ZZ \oplus \ZZ \oplus 12\ZZ} \cong \ZZ/12\ZZ.
.\end{align*}




# 3

The elementary divisors are given by:

\begin{align*}
(x-1)^3 && (x^2+1)^4 && (x+2) \\
(x-1) && (x^2 + 1)^2 && \\
&& (x^2+1)^2 && 
.\end{align*}

The invariant factors are:


\begin{align*}
d_3 &= (x-1)^3(x^2+1)^4(x+2) \\
d_2 &= (x-1)(x^2+1)^2 \\
d_1 &= (x^2+1)^2
.\end{align*}


# 4

**Lemma:**
$(2, x) \normal \ZZ[x]$ is not a principal ideal.

*Proof:*
If this ideal were generated by a single element $p(x)$, then $p \divides 2$ would force $p \in \ZZ$. 
But this means that the element $x\not\in (p)$, a contradiction.
$\qed$

Suppose toward a contradiction that $J = (2, x) \normal \ZZ[x]$ is a direct sum of cyclic submodules of $R \definedas \ZZ[x]$.

Then write 
$$
J = M_1 \oplus M_2 \oplus \cdots \oplus M_n
$$

where each $M_i = \alpha_i \ZZ[x]$ is a cyclic $\ZZ[x]\dash$module. 

Note that by the lemma, we can not have $n=1$, since this would mean $J = \alpha_1 \ZZ[x] = (\alpha_1)$ where we can identify cyclic submodules with principal ideals.

On the other hand, we also can't have $n \geq 2$.
Since the sum is direct, this forces (for example) $M_1 \intersect M_2 = \emptyset$.

However, take the two generating elements $\alpha_1, \alpha_2 \in \ZZ[x]$ and consider their product. 
Noting that $\ZZ[x]$ is a commutative ring, we have

\begin{align*}
\alpha_1 \alpha_2 \in \alpha_1 \ZZ[x] = M_1 \text{ since } \alpha_2 \in \ZZ[x]
\alpha_1 \alpha_2 = \alpha_2 \alpha_1 \in \alpha_2 \ZZ[x] = M_2 \text{ since } \alpha_1 \in \ZZ[x]
,\end{align*}

and so $\alpha_1\alpha_2 \in M_1 \intersect M_2$, a contradiction.
So no such direct sum decomposition is possible.
$\qed$

# 5

**Irreducible:**
Let $a\in M$ be arbitrary; we can then consider the cyclic submodule $aR \normal M$.
Since $M$ is irreducible, we must have $aR = 0$ or $aR = M$.
If $aR = 0$ then $a$ must be $0$. 

Otherwise, $aR = M$ implies that $M$ itself is a cyclic module with generator $a$.
Since $R$ is a PID, we can find an element $p$ such that $\ann_R(M) = (p) \normal R$, in which case $M \cong R/(p)$.

It is also necessarily the case that $(p)$ is maximal, for if there were another ideal $(p) \subseteq J \normal R$, then $J/(p) \normal R/(p) \cong M$ is a submodule by the correspondence theorem for ideals.
But this necessarily forces $J/(p) = 0$ or $M$ by irreducibility of $M$, so $J = (p)$ or $R$.

Thus irreducible modules are exactly the cyclic modules, or equivalently those of the form $R/(p)$ where $(p)$ is a maximal ideal.


**Indecomposable:**
We first note that by the structure theorem for modules over a PID, any module $M$ has a primary decomposition $M \cong \bigoplus_i R/(p_i^{k_i})$.

This means that if $M$ is indecomposable, we must have $M \cong R/(p^n)$ (with a single summand) for some prime $p \in R$; otherwise the primary decomposition would yield additional summands.
Moreover, by the Chinese Remainder Theorem, $M$ can not be decomposed further.

Thus all indecomposable module are of the form $R/(p^n)$ for some $n\geq 1$.



# 6
Suppose $T: V \to V$ is not invertible, then $\dim \im T < n$ and $\dim \ker T > 0$ by the Rank-Nullity theorem.
This means that there is a nontrivial $\vector v \in \ker T$, and a nontrivial vector $\vector w \in \im(T)$, so let $S$ be the matrix formed by the outer product $\vector v \vector w^t$.

We then consider how $ST$ acts on vectors $\vector x$:

\begin{align*}
TS\vector x 
&= T\vector v \vector w^t \vector x  \\
&= (T\vector v )\vector w^t \vector x  \\
&= \vector 0 \vector w^t \vector x \\
&= \mathbf{0_n} \vector x \\
&= \vector 0
,\end{align*}

where $\mathbf{0_n}$ is the $n\times n$ matrix of all zeros.

Similarly,
\begin{align*}
ST\vector x 
&\definedas S \vector y \\
&= \vector v \vector w^t \vector y \\
&= \inner{\vector w}{\vector y} \vector v \\
&= c_i \vector v \\
&\neq \vector 0,
\end{align*}

where $\inner{\vector w}{\vector y} \definedas c_i \neq 0$ because $\vector y \in \im(T) = (\im(T)\perp)\perp$, so $\vector y$ and $\vector w$ can not be orthogonal.

$\qed$

# 7

## a
Note that if $A = 0$ or $I$ then $A$ is patently diagonal, so suppose otherwise. 
Since $A^2 = A$, we have $A^2 - A = 0$ and thus $A$ satisfies the polynomial $p(x) = x^2 - 1 = x(x-1)$.
Moreover, since $A\neq 0, I$, the minimal polynomial is at least degree -- since $p$ is monic, it must in fact be the minimal polynomial.

We can immediately deduce that the size of the largest Jordan block corresponding to $\lambda = 0$ is exactly 1, as is the size of the largest Jordan block corresponding to $\lambda = 1$. 
But this says that *all* Jordan blocks must be size 1, so $JNF(A)$ has no off-diagonal entries and is thus diagonal.

## b
If $k$ is the multiplicity of $\lambda = 0$ as an eigenvalue, we have

\begin{align*}
A \sim 
\left[\begin{array}{ccc|cccc}
0 & 0 & 0     & 0 & 0 & 0 & 0 \\
0 & \ddots & 0     & 0 & 0 & 0 & 0 \\
0 & 0 & 0     & 0 & 0 & 0 & 0 \\ 
\hline
0 & 0 & 0     & 1 & 0 & 0 & 0 \\
0 & 0 & 0     & 0 & 1 & 0 & 0 \\
0 & 0 & 0     & 0 & 0 & \ddots & 0 \\
0 & 0 & 0     & 0 & 0 & 0 & 1
\end{array}\right]
,\end{align*}

which has a $k\times k$ block of zeros and an $(n-k)\times(n-k)$ block of 1s.

# 8

In both cases, we will need the characteristic polynomials $\chi_A(x)$, since $RCF(A)$ will depend on the invariant factors of $A$.
We will also use the fact that over the algebraic closure $\overline \QQ$, the minimal and characteristic polnyomials must have the same roots.

## a
Suppose $m_A(x) = (x-1)(x^2+1)^2$, which is a degree 5 polynomial.
Since $\deg \chi_A$ must be 6 and $m_A$ must divide $\chi_A$ in $\QQ[x]$, the only possibility in this case is that
$$
\chi_A(x) = (x-1)^2 (x^2+2)^2.
$$

To determine the possible invariant factors $\theset{d_i}$, we can just note that $\prod d_i = \chi_A(x)$ and $d_n = m_A(x)$.
With these constraints, the only possibility is

\begin{align*}
d_1 &= (x-1) \\
d_2 &= (x-1)(x^2+1)^2.
,\end{align*}

from which we can immediately obtain the elementary divisors:

\begin{align*}
(x-1), (x-1), (x^2+1)^2
.\end{align*}

Then noting that 
$$
d_2 =d_2 =  (x-1)(x^2+1)^2 = x^5 -x^4 + 4x^3 -4x^2 + 4x - 4,
$$

there is thus only one possible Rational Canonical form:

\begin{align*}
RCF(A) &= 
\left[\begin{array}{c|ccccc}
1 & 0 & 0 & 0 & 0 & 0\\
\hline
0 & 0 & 0 & 0 & 0 & 4 \\
0 & 1 & 0 & 0 & 0 & -4 \\
0 & 0 & 1 & 0 & 0 & 4 \\
0 & 0 & 0 & 1 & 0 & -4 \\
0 & 0 & 0 & 0 & 1 & 1 \\
\end{array}\right]
.\end{align*}

## b 

The constraints $m_A(x) = (x^2+1)^2(x^3+1)$ with $\deg m_A(x) = 7$ and $\deg \chi_A(x) = 10$ forces
$$
\chi_A(x) = (x^2+1)^2 (x^3+1)^2.
$$

Furthermore, the invariant factors are similarly constrained, and so the only possibility is

\begin{align*}
d_1 &= (x_3 + 1) \\
d_2 &= (x^2+1)^2 (x^3+1)
\end{align*}

with corresponding elementary divisors

\begin{align*}
(x^3 + 1), (x^3 + 1), (x^2 + 1)^2
.\end{align*}

Noting that
$$
d_2 = (x^2+1)^2 (x^3+1) = x^5 + x^3 + x^2 + 1,
$$

we have

\begin{align*}
RCF(A) &= 
\left[\begin{array}{cc|ccccc}
0 & -1  & 0 & 0 & 0 & 0 & 0 \\
1 & 0   & 0 & 0 & 0 & 0 & 0 \\ \hline
0 & 0   & 0 & 0 & 0 & 0 & -1 \\
0 & 0   & 1 & 0 & 0 & 0 & 0 \\
0 & 0   & 0 & 1 & 0 & 0 & -1 \\
0 & 0   & 0 & 0 & 1 & 0 & -1 \\
0 & 0   & 0 & 0 & 0 & 1 & 0 \\
\end{array}\right]
.\end{align*}

# 9

The standard computation of $\det(xI - A) = 0$ shows that $\chi_A(x) = \det(xI - A) = (x-1)^2 (x+1)^2$, and so the eigenvalues of $A$ are $1, -1$.
We want the minimal polynomial of $A$, which is given by $\prod(x-\lambda_i)^{\alpha_i}$ where $\alpha_i = \dim E_{\lambda_i}$ is the geometric multiplicity of $\lambda_i$.

Another standard computation shows that 
$$
\lambda = 1 \implies \rank(A - 1I) = 2 \implies \dim \ker (A-1I) = 4-2 = 2
$$
and similarly
$$
\lambda = -1 \implies \rank(A + I) = 3 \implies \dim \ker(A + I) = 4 - 3 = 1.
$$

We thus have
\begin{align*}
p_A(x) &= (x-1) (x+1)^2\\
\chi_A(x) &= (x-1)^2 (x+1)^2
.\end{align*}

To compute $JCF(A)$, we use the following facts:

- For $\lambda = 1$,
  - Since $(x-1)^1$ occurs in $p_A(x)$, the largest Jordan block for $\lambda = 1$ is size 1.
  - Since $(x-1)^2$ occurs in $\chi_A(x)$, the sum of sizes of all such Jordan blocks is 2.
  - Since $\dim E_1 = 2$, there are 2 such Jordan blocks.
- For $\lambda = -1$,
  - Since $(x+1)^2$ occurs in $p_A(x)$, the largest Jordan block for $\lambda = -1$ is size 2.
  - Since $(x+1)^2$ occurs in $\chi_A(x)$, the sum of sizes of all such Jordan blocks is 2.
  - Since $\dim E_{-1} = 1$, there is 1 such Jordan block.


We can thus immediately write

\begin{align*}
JCF(A) = J_{-1}^2 \oplus 2 J_{1}^1 
=
\left[\begin{array}{cccc}
-1 & 1 & 0 & 0 \\
0 & -1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
\end{array}\right]
.\end{align*}


By arguments similar to the previous two problems, the only possible invariant factor decomposition is given by 

\begin{align*}
d_1 &= (x+1) \\
d_2 &= (x-1)^2 (x+1)
\end{align*}

and thus

\begin{align*}
RCF(A) &= C(d_1) \oplus C(d_2) =
\left[\begin{array}{c|ccc}
-1 & 0 & 0 & 0 \\ \hline
0  & 0 & 0 & -1 \\
0  & 1 & 0 & 1 \\
0  & 0 & 1 & 1 \\
\end{array}\right]
.\end{align*}

# 10

Suppose $A^* = A$.
It is then a fact that $A$ is self-adjoint, and so for every $\vector{v}\in V$ we have 
$$
\inner{A\vector{v}}{\vector{v}} =
\inner{\vector{v}}{A^*\vector{v}} =
\inner{\vector{v}}{A\vector{v}}.
$$

## a
Let $(\lambda, \vector v)$ be an eigenvalue of $A$ with one of its corresponding eigenvectors, so $A\vector{v} = \lambda{\vector v}$.

On one hand,
\begin{align*}
\inner{A\vector{v}}{\vector{v}}
=
\inner{\lambda\vector{v}}{\vector{v}}
=
\lambda \inner{\vector{v}}{\vector{v}}
=
\lambda \norm{\vector{v}}^2
,\end{align*}

while on the other hand,

\begin{align*}
\inner{A\vector{v}}{\vector{v}}
=
\inner{\vector{v}}{A^*\vector{v}} 
=
\inner{\vector{v}}{A\vector{v}}
=
\inner{\vector{v}}{\lambda\vector{v}}
=
\overline{\lambda} \inner{\vector{v}}{\vector{v}}
=
\overline{\lambda} \norm{\vector{v}}^2
.\end{align*}

Equating these expressions, we find that
$$
\lambda = \overline{\lambda} \implies \lambda \in \RR. \qed
$$

## b

We can make use of the following fact:

**Theorem (Schur):**
Every square matrix $A \in M_n(\CC)$ is unitarily similar to an upper triangular matrix, i.e. there exists a unitary matrix $U$ such that $A = UTU\inv$ where $T$ is upper-triangular.

Applying this theorem yields $A = UTU\inv$ and thus $T = U\inv A U$. In particular, $A \sim T$.

Noting that if $U$ is unitary then $U\inv = U^*$, we have

\begin{align*}
T^* 
&= (U\inv A U)^* \\
&= U^* A^* (U\inv)^* \\
&= U^* A^* U^{**} \\
&= U\inv A^* U \\
&= T
,\end{align*}

and so $T^* = T$. 

Since $T$ is upper triangular, this forces $T_{ij} = 0$ whenever $i\neq j$
But this makes $T$ diagonal, so $A$ is similar to a diagonal matrix. 
$\qed$


*Proof of Schur's Theorem:*
We'll proceed by induction on $n = \dim_\CC(V)$, and showing that there is an orthonormal basis of $V$ such that the matrix of $A$ is upper triangular.

**Lemma:** 
If $V$ is finite dimensional and $\lambda$ is an eigenvalue of $A$, then $\overline{\lambda}$ is an eigenvalue of $A^*$.

*Proof:*
$$
\operatorname{det}(A-\lambda I) = 
0 = \overline{\operatorname{det}\left(A^{*} - \bar{\lambda} I\right)}. \qed
$$


Since $\CC$ is algebraically closed, every matrix $A \in M_n(\CC)$ will have an eigenvalue, since its characteristic polynomial will have a root by the Fundamental Theorem of Algebra.

So let $\lambda_1, \vector v_1$ be an eigenvalue/eigenvector pair of the adjoint $A^*$.


Consider the space $S = \spanof_\CC\theset{\vector v_1}$; then $V = S \oplus S^\perp$.
The claim is that the original $A$ will restrict to an operator on $S^\perp$, which has dimension $n-1$.
The inductive hypothesis will then apply to $\restrictionof{A}{S^\perp}$.

Note that if this holds, there will be an orthonormal basis $\mathcal{B}$ of $S^\perp$ such that the matrix
$$
\mathbf{A}' \definedas [\restrictionof{A}{S^\perp}]_{\mathcal{B}}
$$ 
will be upper triangular. 
We would then be able to obtain an orthonormal basis 
$\mathcal{C} \definedas \mathcal{B} \union \theset{\vector{v_1}}$ of $S \oplus S^\perp = V$.

Since we have a direct sum decomposition, the matrix of $A$ with respect to $\mathcal{C}$ can be written in block form as 

\begin{align*}
[A]_{\mathcal{C}} &=
\left[\begin{array}{cc}
[\restrictionof{A}{S}]_{\mathcal{C}} & 0 \\
0 & [\restrictionof{A}{S^\perp}]_{\mathcal{C}}
\end{array}\right]
=
\left[\begin{array}{cc}
[\restrictionof{A}{S}]_{\theset{\vector v_1}} & 0 \\
0 & [\restrictionof{A}{S^\perp}]_{\mathcal{B}}
\end{array}\right]
=
\left[\begin{array}{cc}
\lambda_1 & 0 \\
0 & \mathbf{A}' 
\end{array}\right]
,\end{align*}

which is upper-triangular since $\mathbf{A}'$ is upper-triangular.

To see that $A$ does indeed restrict to an operator on $S^\perp$, we need to show that $A(S^\perp) \subseteq S^\perp$.
So let $\vector s \in S^\perp$; then $\inner{\vector v_1}{\vector s} = 0$ by definition.
Then $A\vector s \in S^\perp$ since

\begin{align*}
\inner{\vector v_1}{A\vector s} 
&= \inner{A^* \vector v_1}{\vector s} \\
&= \inner{\lambda_1 \vector v_1}{\vector s} \\
&= \lambda_1 \inner{\vector v_1}{\vector s} \\
&= 0
.\end{align*}

$\qed$
